from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# -------------------------
# 1. Start SparkSession
# -------------------------
spark = SparkSession.builder \
    .appName("Day13_PySpark_Grouping_Joins") \
    .getOrCreate()

print("âœ… SparkSession started!")

# -------------------------
# 2. Load Sample Data
# -------------------------
employees = spark.read.option("header", "true").option("inferSchema", "true").csv("data/employees.csv")
departments = spark.read.option("header", "true").option("inferSchema", "true").csv("data/departments.csv")

print("Employees Data:")
employees.show()

print("Departments Data:")
departments.show()

# -------------------------
# 3. groupBy()
# -------------------------
print("ðŸ‘‰ Count employees per department:")
employees.groupBy("department").count().show()

# -------------------------
# 4. agg()
# -------------------------
print("ðŸ‘‰ Average and Max salary per department:")
employees.groupBy("department").agg(
    F.avg("salary").alias("avg_salary"),
    F.max("salary").alias("max_salary")
).show()

# -------------------------
# 5. count()
# -------------------------
print("ðŸ‘‰ Total number of employees:")
print("Count =", employees.count())

# -------------------------
# 6. joins
# -------------------------
print("ðŸ‘‰ Inner Join Employees with Departments:")
employees.join(departments, employees["department"] == departments["dept_id"], "inner").show()

print("ðŸ‘‰ Left Join:")
employees.join(departments, employees["department"] == departments["dept_id"], "left").show()

print("ðŸ‘‰ Full Outer Join:")
employees.join(departments, employees["department"] == departments["dept_id"], "outer").show()

# -------------------------
# 7. Stop Spark
# -------------------------
spark.stop()
print("âœ… SparkSession stopped!")

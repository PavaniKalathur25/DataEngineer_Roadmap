---

### üìì **notebooks/Day55_AzureEventHub_Integration.ipynb**

```python
# üß© Day 55 - Azure Event Hub Integration with Apache Spark

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, avg
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType

# 1Ô∏è‚É£ Create Spark Session
spark = SparkSession.builder.appName("AzureEventHubIntegration").getOrCreate()

# 2Ô∏è‚É£ Event Hub connection
connectionString = "Endpoint=sb://<YourNamespace>.servicebus.windows.net/;SharedAccessKeyName=<KeyName>;SharedAccessKey=<Key>;EntityPath=<EventHubName>"
ehConf = {"eventhubs.connectionString": connectionString}

# 3Ô∏è‚É£ Read data stream from Event Hub
raw_df = spark.readStream.format("eventhubs").options(**ehConf).load()

# 4Ô∏è‚É£ Define event schema
schema = StructType([
    StructField("deviceId", StringType()),
    StructField("temperature", DoubleType()),
    StructField("timestamp", TimestampType())
])

# 5Ô∏è‚É£ Parse message body
parsed_df = raw_df.select(from_json(col("body").cast("string"), schema).alias("data")).select("data.*")

# 6Ô∏è‚É£ Add simple transformation (avg temp by minute)
transformed_df = parsed_df.groupBy("deviceId").avg("temperature").withColumnRenamed("avg(temperature)", "avg_temp")

# 7Ô∏è‚É£ Write to console (can be ADLS / DB instead)
query = transformed_df.writeStream \
    .outputMode("complete") \
    .format("console") \
    .option("truncate", "false") \
    .start()

query.awaitTermination()

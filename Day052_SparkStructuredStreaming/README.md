# Day 52 â€“ Spark Structured Streaming (ReadStream & WriteStream)

## ğŸ”¹ What is Structured Streaming?
Structured Streaming is a scalable and fault-tolerant stream processing engine built on top of Spark SQL.  
It allows continuous, real-time processing of incoming data using the **same DataFrame API** as batch jobs.

---

## ğŸ”¹ Where Itâ€™s Used
- Real-time log monitoring
- IoT sensor data ingestion
- Kafka stream processing
- ETL pipelines to Data Lakes (Azure, AWS, GCP)
- Streaming dashboards (Power BI, Databricks)

**Example:**  
Live order data flows from Kafka â†’ Spark processes â†’ writes to Azure Data Lake â†’ Power BI dashboard updates instantly.

---

## ğŸ”¹ Why Itâ€™s Used
| Benefit | Description |
|----------|--------------|
| âš¡ Continuous Processing | Data is processed as it arrives |
| ğŸ§± Unified API | Same code for batch and streaming |
| ğŸ’¾ Fault Tolerance | Auto recovery using checkpoints |
| ğŸ” Integration | Works with Kafka, S3, ADLS, Delta Lake |
| ğŸ“ˆ Scalability | Handles millions of events per second |

---

## ğŸ”¹ Core Components & Keywords

| Keyword | Description |
|----------|--------------|
| **readStream** | Reads data continuously from a streaming source |
| **writeStream** | Writes output continuously to a sink |
| **Trigger** | Defines processing interval |
| **Output Mode** | Controls how results are written: `append`, `update`, `complete` |
| **Checkpoint** | Saves stream state for fault recovery |
| **Source** | Streaming input (Kafka, socket, files) |
| **Sink** | Output target (console, file, DB) |
| **Watermark** | Handles late-arriving data in event-time windows |

---

## ğŸ”¹ Summary
Structured Streaming makes your data pipeline **live and continuous**.  
You can **read (ReadStream)** â†’ **transform** â†’ **write (WriteStream)** data, enabling real-time analytics without complex setup.

---

## ğŸ§° Coding Examples
See `readstream_example.py` and `writestream_example.py` for hands-on examples.

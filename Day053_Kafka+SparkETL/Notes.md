# Day 53 â€“ Kafka + Spark ETL Quick Notes

## ðŸ”¸ Pipeline Flow
Kafka (Producer) â†’ Kafka Topic â†’ Spark Structured Streaming â†’ Transform â†’ Sink (Console, File, DB)

## ðŸ”¸ Typical Use Case
E-commerce order streaming:
- Producer sends new orders to Kafka  
- Spark reads them in real time  
- Aggregates sales by region and writes to dashboard  

## ðŸ”¸ Common Transformations
- Filtering irrelevant records  
- Parsing JSON/CSV data  
- Aggregating with groupBy()  
- Joining with reference data  
- Window functions for time-based aggregations  

## ðŸ”¸ Example Sinks
- Console (for testing)  
- Azure Data Lake / S3  
- Delta tables for real-time analytics  
- Databases (PostgreSQL, Snowflake)  

## ðŸ”¸ Keywords to Remember
Kafka | Spark Structured Streaming | ETL | Schema | Checkpoint | Sink | Source | Micro-batch | Fault Tolerance | Real-time Pipeline

ðŸ’¡ Tip: Add `.trigger(processingTime='1 minute')` to control micro-batch intervals.

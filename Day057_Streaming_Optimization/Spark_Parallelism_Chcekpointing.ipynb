# üß© Day 57: Streaming Optimization ‚Äì Parallelism & Checkpointing

# -------------------------------------------------------
# 1Ô∏è‚É£ Initialize Spark Session
# -------------------------------------------------------
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("StreamingOptimization_Day57") \
    .config("spark.sql.shuffle.partitions", 8) \
    .config("spark.default.parallelism", 8) \
    .getOrCreate()

print("‚úÖ Spark Session Initialized")
print("Shuffle Partitions:", spark.conf.get("spark.sql.shuffle.partitions"))
print("Default Parallelism:", spark.conf.get("spark.default.parallelism"))

# -------------------------------------------------------
# 2Ô∏è‚É£ Load Sample Streaming Data
# -------------------------------------------------------
# Simulate streaming JSON data
sample_data = [
    {"event": "click", "value": 10},
    {"event": "view", "value": 20},
    {"event": "click", "value": 15},
    {"event": "purchase", "value": 5}
]

import json

with open("data/sample_stream_data.json", "w") as f:
    for record in sample_data:
        f.write(json.dumps(record) + "\n")

print("‚úÖ Sample streaming data written to 'data/sample_stream_data.json'")

# -------------------------------------------------------
# 3Ô∏è‚É£ Configure Parallelism for Transformation
# -------------------------------------------------------
df = spark.read.json("data/sample_stream_data.json")

print("üßæ Input Data:")
df.show()

# Group and aggregate by event
aggregated = df.groupBy("event").sum("value")

print("‚öôÔ∏è Aggregated Output:")
aggregated.show()

# Check how many partitions Spark used internally
print("Number of partitions used:", aggregated.rdd.getNumPartitions())


# -------------------------------------------------------
# 4Ô∏è‚É£ Simulate Streaming with Checkpointing
# -------------------------------------------------------
from pyspark.sql.functions import col

# Convert static DF to streaming source (for demo)
streaming_input = spark.readStream \
    .schema("event STRING, value INT") \
    .json("data/")

checkpoint_dir = "checkpoint/day57_cp"

query = streaming_input.groupBy("event").sum("value") \
    .writeStream \
    .format("console") \
    .outputMode("complete") \
    .option("checkpointLocation", checkpoint_dir) \
    .trigger(processingTime="10 seconds") \
    .start()

print("üöÄ Streaming started with checkpointing...")
print(f"Checkpoint directory: {checkpoint_dir}")

# Wait for a few micro-batches
query.awaitTermination(30)

# -------------------------------------------------------
# 5Ô∏è‚É£ Validate Checkpointing Recovery
# -------------------------------------------------------
# Stop and restart the stream to simulate recovery
query.stop()
print("‚èπÔ∏è Streaming stopped. Now restarting to test recovery...")

# Restart with same checkpoint location
query2 = streaming_input.groupBy("event").sum("value") \
    .writeStream \
    .format("console") \
    .outputMode("complete") \
    .option("checkpointLocation", checkpoint_dir) \
    .trigger(processingTime="10 seconds") \
    .start()

print("üîÑ Streaming restarted with previous checkpoint state.")
query2.awaitTermination(20)
query2.stop()

# -------------------------------------------------------
# 6Ô∏è‚É£ Summary of Learnings
# -------------------------------------------------------
summary = {
    "Parallelism": "Helps distribute work across executors/cores to speed up transformations.",
    "Checkpointing": "Saves intermediate streaming state for fault-tolerance and recovery.",
    "Optimization_Tips": [
        "Use spark.sql.shuffle.partitions wisely based on cluster size.",
        "Use checkpointLocation for all long-running streams.",
        "Monitor streaming query metrics using Spark UI."
    ]
}

for key, value in summary.items():
    print(f"üîπ {key}: {value}")
